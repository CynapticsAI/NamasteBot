# -*- coding: utf-8 -*-
"""Copy of IITI_GPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmo5TWppM8o6_TidaEUHgDku-VpS79nu
"""

!pip install -q --upgrade langchain-community
!pip install openai
!pip install git+https://github.com/openai/whisper.git
!pip install google-auth
!pip install faiss-cpu
!pip install edge-tts

import os
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
llm = ChatOpenAI(model_name="gpt-4-turbo", temperature=0.7)
loader = TextLoader("/content/drive/MyDrive/IITI GPT/last data.txt")
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
vectorstore = FAISS.from_documents(docs, embeddings)

#Taken from Deepseek
# Install required library
!pip install pydub -q

# Import dependencies
from IPython.display import HTML, Javascript, Audio
from google.colab import output
from base64 import b64decode
from pydub import AudioSegment

# Define JavaScript/HTML interface
js_code = """
<div id="recorder">
  <button onclick="startRecording()" id="startBtn">üé§ Start Recording</button>
  <button onclick="stopRecording()" id="stopBtn" disabled>‚èπÔ∏è Stop & Save</button>
</div>

<script>
let mediaRecorder;
let audioChunks = [];

async function startRecording() {
  document.getElementById("stopBtn").disabled = false;
  document.getElementById("startBtn").disabled = true;

  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(stream);

    mediaRecorder.ondataavailable = (event) => {
      audioChunks.push(event.data);
    };

    mediaRecorder.start();
  } catch (error) {
    alert('Error accessing microphone: ' + error.message);
  }
}

async function stopRecording() {
  document.getElementById("startBtn").disabled = false;
  document.getElementById("stopBtn").disabled = true;

  mediaRecorder.stop();
  mediaRecorder.onstop = async () => {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
    const reader = new FileReader();

    reader.readAsDataURL(audioBlob);
    reader.onloadend = () => {
      const base64data = reader.result;
      google.colab.kernel.invokeFunction('notebook.saveAudio', [base64data], {});
    };

    audioChunks = [];
  };
}
</script>
"""

# Audio processing callback
def save_audio(base64_data):
    try:
        # Decode base64 audio data
        audio_bytes = b64decode(base64_data.split(',')[1])

        # Save original webm file
        with open('my_recording.webm', 'wb') as f:
            f.write(audio_bytes)

        # Convert to WAV format
        audio = AudioSegment.from_file('my_recording.webm', format='webm')
        audio.export('my_recording.wav', format='wav')

        # Show audio player
        display(Audio('my_recording.wav'))
        print("‚úÖ Audio saved as my_recording.wav")

    except Exception as e:
        print("Error processing audio:", e)

# Register callback
output.register_callback('notebook.saveAudio', save_audio)

# Display the recorder interface
display(HTML(js_code))

# # Optional: Add download button
# def add_download():
#     display(HTML(
#         '''<div style="margin-top:20px">
#            <a href="my_recording.wav" download>
#              <button>‚¨áÔ∏è Download WAV File</button>
#            </a>
#            </div>'''
#     ))

# # Call this after saving audio to show download button
# add_download()

import whisper
query=whisper.load_model("medium").transcribe("my_recording.wav",language="en")['text']
print(query)

from langchain.chains import RetrievalQA
retriever = vectorstore.as_retriever(search_kwargs={"k": 8},similarity_score_threshold=0.7)
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)
response = qa_chain.invoke({"query": query})
print("\nGenerated Response:\n", response["result"])

from IPython.display import Audio
import edge_tts

# Your text
text = response["result"]
text = text.replace("**", " ")
text = text.replace("*", " ")
text = text.replace("_", " ")
# Customize voice and speech parameters
VOICE = "en-US-ChristopherNeural"  # Deep male voice
RATE = "+10%"  # Speed adjustment (-20% to +20% for natural pacing)
PITCH = "+5Hz"  # Slightly lower pitch for warmth

# Generate speech with adjustments
async def generate_speech():
    communicate = edge_tts.Communicate(text, VOICE, rate=RATE, pitch=PITCH)
    await communicate.save("human_like_audio.mp3")

# Run in Colab/Jupyter (no asyncio loop needed)
await generate_speech()

# Auto-play enhanced audio
Audio("human_like_audio.mp3", autoplay=True)